{"cells":[{"cell_type":"markdown","metadata":{},"source":["# PySpark Read and Write JSON file into GCS Bucket in GCP\n","\n","1. PySpark SQL provides ***read.json(\"path\")*** to read a single line or multiline (multiple lines) JSON file into PySpark DataFrame and ***write.json(\"path\")*** to save or write to JSON file."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n","from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DoubleType, BooleanType\n","\n","# Create SparkSession from builder\n","import pyspark\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local\") \\\n","                    .appName('Cloud and AI Analytics') \\\n","                    .getOrCreate()"]},{"cell_type":"markdown","metadata":{},"source":["### Simple JSON"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["path = \"gs://bqdemogcp-pde/json_input/gcsdata.json\"  #gcsdata.json\n","bucket_name = \"bqdemogcp-pde\""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Read JSON file into dataframe\n","df = spark.read.format(\"json\") \\\n","    .option(\"inferSchema\", True)\\\n","    .option(\"header\", True)\\\n","    .load(path)"]},{"cell_type":"markdown","metadata":{},"source":["When Used, format(\"json\") method, you can also specify the Data sources by their fully qualified name as below."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+------------+-----+-----------+-------+\n","|       City|RecordNumber|State|ZipCodeType|Zipcode|\n","+-----------+------------+-----+-----------+-------+\n","|PARC PARQUE|           1|   PR|   STANDARD|    704|\n","+-----------+------------+-----+-----------+-------+\n","\n"]}],"source":["df.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Reading files with a user-specified custom schema"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["path1 = \"gs://bqdemogcp-pde/jsondata.json\"   #jsondata.json"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["custom_schema = StructType([\n","    StructField(\"schema\", StringType(), False),\n","    StructField(\"id\", StringType(), True),\n","    StructField(\"app\", StringType(), True),\n","    StructField(\"description\", StringType(), True)])"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["df = spark.read.format(\"json\") \\\n","    .schema(custom_schema) \\\n","    .load(path1)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+---+---+-----------+\n","|schema| id|app|description|\n","+------+---+---+-----------+\n","|     a|  1|foo|       test|\n","|     b|  2|bar|      test2|\n","+------+---+---+-----------+\n","\n"]}],"source":["df.show()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- schema: string (nullable = true)\n"," |-- id: string (nullable = true)\n"," |-- app: string (nullable = true)\n"," |-- description: string (nullable = true)\n","\n"]}],"source":["df.printSchema()"]},{"cell_type":"markdown","metadata":{},"source":["### When you use format(\"json\") method, you can also specify the Data sources by their fully qualified name as below."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- app: string (nullable = true)\n"," |-- description: string (nullable = true)\n"," |-- id: string (nullable = true)\n"," |-- schema: string (nullable = true)\n","\n","+---+-----------+---+------+\n","|app|description| id|schema|\n","+---+-----------+---+------+\n","|foo|       test|  1|     a|\n","|bar|      test2|  2|     b|\n","+---+-----------+---+------+\n","\n"]}],"source":["# Read JSON file into dataframe\n","df = spark.read.format('org.apache.spark.sql.json').option(\"mode\", \"PERMISSIVE\").load(path1)\n","df.printSchema()\n","df.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Read JSON file from multiline\n","\n","1. PySpark JSON data source provides multiple options to read files in different options, use multiline option to read JSON files scattered across multiple lines. \n","2. By default multiline option, is set to false."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["multiline_df = \"gs://bqdemogcp-pde/multilinedata.json\"  #multilinedata.json"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Read multiline json file\n","multiline_df = spark.read.format(\"json\") \\\n","    .option(\"inferSchema\", True)\\\n","    .option(\"header\", True)\\\n","    .load(multiline_df)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+------------+-----+-----------+-------+---------------+\n","|               City|RecordNumber|State|ZipCodeType|Zipcode|_corrupt_record|\n","+-------------------+------------+-----+-----------+-------+---------------+\n","|               null|        null| null|       null|   null|              [|\n","|PASEO COSTA DEL SUR|           2|   PR|   STANDARD|    704|           null|\n","|       BDA SAN LUIS|          10|   PR|   STANDARD|    709|           null|\n","|               null|        null| null|       null|   null|              ]|\n","+-------------------+------------+-----+-----------+-------+---------------+\n","\n"]}],"source":["multiline_df.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Reading multiple files at a time\n","\n","1. Using the read.json() method you can also read multiple JSON files from different gcs paths, just pass all file names with fully qualified paths by separating comma"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Read multiple files\n","df = spark.read.format(\"json\") \\\n","    .option(\"inferSchema\", True)\\\n","    .option(\"header\", True)\\\n","    .load([\"gs://bqdemogcp-pde/json_input/gcsdata.json\", \"gs://bqdemogcp-pde/json_input/gcsdata1.json\"])  #gcsdata.json, gcsdata1.json"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- City: string (nullable = true)\n"," |-- RecordNumber: long (nullable = true)\n"," |-- State: string (nullable = true)\n"," |-- ZipCodeType: string (nullable = true)\n"," |-- Zipcode: long (nullable = true)\n","\n"]}],"source":["df.printSchema()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+------------+-----+-----------+-------+\n","|       City|RecordNumber|State|ZipCodeType|Zipcode|\n","+-----------+------------+-----+-----------+-------+\n","|PARC PARQUE|           1|   PR|   STANDARD|    704|\n","|PARC PARQUE|           2|   IN|    PREMIUM|    705|\n","+-----------+------------+-----+-----------+-------+\n","\n"]}],"source":["df.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Reading all files in a directory\n","\n","1. Read all JSON files from a directory into DataFrame just by passing gcs directory as a path to the json() method."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# Read all JSON files from a folder\n","df = spark.read.format(\"json\") \\\n","    .option(\"inferSchema\", True)\\\n","    .option(\"header\", True)\\\n","    .load(\"gs://bqdemogcp-pde/json_input/*.json\")  #json_input - folder"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- City: string (nullable = true)\n"," |-- RecordNumber: long (nullable = true)\n"," |-- State: string (nullable = true)\n"," |-- ZipCodeType: string (nullable = true)\n"," |-- Zipcode: long (nullable = true)\n","\n"]}],"source":["df.printSchema()"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+------------+-----+-----------+-------+\n","|       City|RecordNumber|State|ZipCodeType|Zipcode|\n","+-----------+------------+-----+-----------+-------+\n","|PARC PARQUE|           1|   PR|   STANDARD|    704|\n","|PARC PARQUE|           2|   IN|    PREMIUM|    705|\n","+-----------+------------+-----+-----------+-------+\n","\n"]}],"source":["df.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Write JSON data to GCS bucket"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n","    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n","    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n","    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n","    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n","  ]"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["schema = StructType([ \\\n","    StructField(\"firstname\",StringType(),True), \\\n","    StructField(\"middlename\",StringType(),True), \\\n","    StructField(\"lastname\",StringType(),True), \\\n","    StructField(\"id\", StringType(), True), \\\n","    StructField(\"gender\", StringType(), True), \\\n","    StructField(\"salary\", IntegerType(), True) \\\n","  ])"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["pysparkDF = spark.createDataFrame(data=data2,schema=schema)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- firstname: string (nullable = true)\n"," |-- middlename: string (nullable = true)\n"," |-- lastname: string (nullable = true)\n"," |-- id: string (nullable = true)\n"," |-- gender: string (nullable = true)\n"," |-- salary: integer (nullable = true)\n","\n","+---------+----------+--------+-----+------+------+\n","|firstname|middlename|lastname|id   |gender|salary|\n","+---------+----------+--------+-----+------+------+\n","|James    |          |Smith   |36636|M     |3000  |\n","|Michael  |Rose      |        |40288|M     |4000  |\n","|Robert   |          |Williams|42114|M     |4000  |\n","|Maria    |Anne      |Jones   |39192|F     |4000  |\n","|Jen      |Mary      |Brown   |     |F     |-1    |\n","+---------+----------+--------+-----+------+------+\n","\n"]}],"source":["pysparkDF.printSchema()\n","pysparkDF.show(truncate=False)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["gs://bqdemogcp-pde/json_output/data1.json\n"]}],"source":["print(\"gs://{}/{}/data1.json\".format(bucket_name, \"json_output\"))"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["#write to gcs bucket\n","pysparkDF.repartition(1).write.mode(\"overwrite\").format('json').save(\"gs://{}/{}/employee.json\".format(bucket_name, \"json_output\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#df.write.mode('Overwrite').json(\"gs://{}/{}/data.json\".format(bucket_name, \"json_output\"))"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":4}