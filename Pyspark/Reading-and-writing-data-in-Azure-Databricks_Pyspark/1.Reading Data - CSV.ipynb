{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magic Command: &percnt;sh\n",
    "For example, **&percnt;sh** allows us to execute shell commands on the driver\n",
    "\n",
    "On databricks we need to use %ms to mark it as a markdown cell"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%sh ps | grep 'java'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magic Command: %run\n",
    "## You can run a notebook from another notebook by using the Magic Command %run\n",
    "### All variables & functions defined in that other notebook will become available in your current notebook\n",
    "### For example, The following cell should fail to execute because the variable username has not yet been declared:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/solliancenet/microsoft-learning-paths-databricks-notebooks/blob/master/data-engineering/DBC/01-Introduction-to-Azure-Databricks.dbc?raw=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark API (Python)\n",
    "\n",
    "0. Select **Spark Python API (Sphinx)**.\n",
    "0. Look up the documentation for `pyspark.sql.SparkSession`.\n",
    "  0. In the lower-left-hand-corner type **SparkSession** into the search field.\n",
    "  0. Hit **[Enter]**.\n",
    "  0. The search results should appear in the right-hand pane.\n",
    "  0. Click on **pyspark.sql.SparkSession (Python class, in pyspark.sql module)**\n",
    "  0. The documentation should open in the right-hand pane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #1 - Read The CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A reference to our tab-separated-file\n",
    "csvFile = \"./sales_info.csv\"\n",
    "\n",
    "tempDF = (spark.read           # The DataFrameReader\n",
    "                               # .option(\"sep\", \"\\t\") Use tab delimiter (default is comma-separator)\n",
    "   .csv(csvFile)               # Creates a DataFrame from CSV after reading in the file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tempDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|    _c0|    _c1|  _c2|\n",
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "|   GOOG|    Sam|  200|\n",
      "|   GOOG|Charlie|  120|\n",
      "|   GOOG|  Frank|  340|\n",
      "|   MSFT|   Tina|  600|\n",
      "+-------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #2 - Use the File's Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (spark.read                    # The DataFrameReader\n",
    "          # Use tab delimiter (default is comma-separator)\n",
    "   .option(\"header\", \"true\")   # Use first line of all files as header\n",
    "   .csv(csvFile)               # Creates a DataFrame from CSV after reading in the file\n",
    "   .printSchema()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark.read                        # The DataFrameReader\n",
    "   .option(\"header\", \"true\")       # Use first line of all files as header\n",
    "   #.option(\"sep\", \"\\t\")            # Use tab delimiter (default is comma-separator)\n",
    "   .option(\"inferSchema\", \"true\")  # Automatically infer data types\n",
    "   .csv(csvFile)                   # Creates a DataFrame from CSV after reading in the file\n",
    "   .printSchema()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
