{"cells":[{"cell_type":"markdown","source":["# Horovod\n\nHorovodRunner is a general API to run distributed DL workloads on Databricks using Uber’s [Horovod](https://github.com/uber/horovod) framework. By integrating Horovod with Spark’s barrier mode, Databricks is able to provide higher stability for long-running deep learning training jobs on Spark. HorovodRunner takes a Python method that contains DL training code with Horovod hooks. This method gets pickled on the driver and sent to Spark workers. A Horovod MPI job is embedded as a Spark job using barrier execution mode. The first executor collects the IP addresses of all task executors using BarrierTaskContext and triggers a Horovod job using mpirun. Each Python MPI process loads the pickled program back, deserializes it, and runs it.\n\n<br>\n\n![](https://files.training.databricks.com/images/horovod-runner.png)\n\nFor additional resources, see:\n* [Horovod Runner Docs](https://docs.microsoft.com/en-us/azure/databricks/applications/deep-learning/distributed-training/horovod-runner)\n* [Horovod Runner webinar](https://vimeo.com/316872704/e79235f62c)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"775a56f4-2239-468c-aa21-881906036d0e"}}},{"cell_type":"markdown","source":["Run the following cell to set up our environment."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66cda9af-f842-4786-aa94-ed9b232a1ee4"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c47df75-89f6-4ae5-b729-0af95e0f98ee"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Build Model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3cb5903-266e-4e46-aa4a-775fa08fb437"}}},{"cell_type":"code","source":["import numpy as np\nnp.random.seed(0)\nimport tensorflow as tf\ntf.set_random_seed(42) # For reproducibility\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\ndef build_model():\n  return Sequential([Dense(20, input_dim=8, activation='relu'),\n                    Dense(20, activation='relu'),\n                    Dense(1, activation='linear')]) # Keep the output layer as linear because this is a regression problem"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e3fae6f-a139-4968-b43d-63fd4bcbe4fb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Shard Data\n\nFrom the [Horovod docs](https://github.com/horovod/horovod/blob/master/docs/concepts.rst):\n\nHorovod core principles are based on the MPI concepts size, rank, local rank, allreduce, allgather, and broadcast. These are best explained by example. Say we launched a training script on 4 servers, each having 4 GPUs. If we launched one copy of the script per GPU:\n\n* Size would be the number of processes, in this case, 16.\n\n* Rank would be the unique process ID from 0 to 15 (size - 1).\n\n* Local rank would be the unique process ID within the server from 0 to 3.\n\nWe need to shard our data across our processes.  **NOTE:** We are using a Pandas DataFrame for demo purposes. In the next notebook we will use Parquet files with Petastorm for better scalability."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7c31c95-eea4-4b5c-a32e-bae0b2c84f96"}}},{"cell_type":"code","source":["from sklearn.datasets.california_housing import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndef get_dataset(rank=0, size=1):\n  scaler = StandardScaler()\n  cal_housing = fetch_california_housing(data_home=\"/dbfs/ml/\" + str(rank) + \"/\")\n  X_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n                                                       cal_housing.target,\n                                                       test_size=0.2,\n                                                       random_state=1)\n  scaler.fit(X_train)\n  X_train = scaler.transform(X_train[rank::size])\n  y_train = y_train[rank::size]\n  X_test = scaler.transform(X_test[rank::size])\n  y_test = y_test[rank::size]\n  return (X_train, y_train), (X_test, y_test)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f958472b-4ba5-4dc3-b3ed-84bd1926b2c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Horovod"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"847b7777-0482-4bea-8d54-c20d2cda6cec"}}},{"cell_type":"code","source":["from tensorflow.keras import optimizers\nimport horovod.tensorflow.keras as hvd\nfrom keras import backend as K\n\ndef run_training_horovod():\n  # Horovod: initialize Horovod.\n  hvd.init()\n  # If using GPU: pin GPU to be used to process local rank (one GPU per process)\n  # config = tf.ConfigProto()\n  # config.gpu_options.allow_growth = True\n  # config.gpu_options.visible_device_list = str(hvd.local_rank())\n  # K.set_session(tf.Session(config=config))\n  print(f\"Rank is: {hvd.rank()}\")\n  print(f\"Size is: {hvd.size()}\")\n  \n  (X_train, y_train), (X_test, y_test) = get_dataset(hvd.rank(), hvd.size())\n  \n  model = build_model()\n  \n  from tensorflow.keras import optimizers\n  # Horovod: adjust learning rate based on number of GPUs/CPUs.\n  optimizer = optimizers.Adam(lr=0.001*hvd.size())\n  \n  # Horovod: add Horovod Distributed Optimizer.\n  optimizer = hvd.DistributedOptimizer(optimizer)\n\n  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n  \n  history = model.fit(X_train, y_train, validation_split=.2, epochs=10, batch_size=64, verbose=2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cedc96bb-2bf8-4805-a9da-06f73bf56533"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Test it out on just the driver."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d05f35e6-6a72-4ee3-8e59-44015f0999cd"}}},{"cell_type":"code","source":["from sparkdl import HorovodRunner\nhr = HorovodRunner(np=-1)\nhr.run(run_training_horovod)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"440ef8f1-7b65-4199-ae89-d24e5d480c9f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Better Horovod"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73a8be20-c71c-4aa0-8631-ce634d5d85ab"}}},{"cell_type":"code","source":["from tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import *\n\ndef run_training_horovod():\n  # Horovod: initialize Horovod.\n  hvd.init()\n  # If using GPU: pin GPU to be used to process local rank (one GPU per process)\n  # config = tf.ConfigProto()\n  # config.gpu_options.allow_growth = True\n  # config.gpu_options.visible_device_list = str(hvd.local_rank())\n  # K.set_session(tf.Session(config=config))\n  \n  \n  \n  print(f\"Rank is: {hvd.rank()}\")\n  print(f\"Size is: {hvd.size()}\")\n  \n  (X_train, y_train), (X_test, y_test) = get_dataset(hvd.rank(), hvd.size())\n  \n  model = build_model()\n  \n  from tensorflow.keras import optimizers\n  # Horovod: adjust learning rate based on number of GPUs.\n  optimizer = optimizers.Adam(lr=0.001*hvd.size())\n  \n  # Horovod: add Horovod Distributed Optimizer.\n  optimizer = hvd.DistributedOptimizer(optimizer)\n\n  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n\n  # Use the optimized FUSE Mount\n  checkpoint_dir = f\"{ml_working_path}/horovod_checkpoint_weights.ckpt\"\n  \n  callbacks = [\n    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n    # This is necessary to ensure consistent initialization of all workers when\n    # training is started with random weights or restored from a checkpoint.\n    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n\n    # Horovod: average metrics among workers at the end of every epoch.\n    # Note: This callback must be in the list before the ReduceLROnPlateau,\n    # TensorBoard or other metrics-based callbacks.\n    hvd.callbacks.MetricAverageCallback(),\n\n    # Horovod: using `lr = 1.0 * hvd.size()` from the very beginning leads to worse final\n    # accuracy. Scale the learning rate `lr = 1.0` ---> `lr = 1.0 * hvd.size()` during\n    # the first five epochs. See https://arxiv.org/abs/1706.02677 for details.\n    hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n    \n    # Reduce the learning rate if training plateaus.\n    ReduceLROnPlateau(patience=10, verbose=1)\n  ]\n  \n  # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n  if hvd.rank() == 0:\n    callbacks.append(ModelCheckpoint(checkpoint_dir, save_weights_only=True))\n  \n  history = model.fit(X_train, y_train, validation_split=.2, epochs=10, batch_size=64, verbose=2, callbacks=callbacks)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51c3e36d-9f9f-480d-93d1-fd5b6d8ba0fe"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Test it out on just the driver."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bcd9f180-0530-4eb1-b773-441c1f3708c3"}}},{"cell_type":"code","source":["from sparkdl import HorovodRunner\nhr = HorovodRunner(np=-1)\nhr.run(run_training_horovod)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be4a9170-da6b-45b6-a09c-073210930087"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Run on all workers"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f8f0dd9-698b-4cd0-9b4e-0f261db8937c"}}},{"cell_type":"code","source":["from sparkdl import HorovodRunner\nhr = HorovodRunner(np=0)\nhr.run(run_training_horovod)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82393b05-faa7-4dba-8e80-2ae384ca843a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1. Horovod","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3013059022292408}},"nbformat":4,"nbformat_minor":0}
