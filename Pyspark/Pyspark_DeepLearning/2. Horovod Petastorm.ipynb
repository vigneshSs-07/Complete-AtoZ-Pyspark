{"cells":[{"cell_type":"markdown","source":["# Horovod with Petastorm\n\n[Petastorm](https://github.com/uber/petastorm) enables single machine or distributed training and evaluation of deep learning models from datasets in Apache Parquet format. It supports ML frameworks such as TensorFlow, Pytorch, and PySpark and can be used from pure Python code.\n\n**Required Libraries**: \n* `petastorm==0.8.2` via PyPI"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38bac1e6-2ecc-415e-a588-47274d8edcc1"}}},{"cell_type":"markdown","source":["Run the following cell to set up our environment."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82cebffd-c0c5-44da-a5a1-860e8452813d"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8cba74c8-0369-44f9-8a03-b9c4dca8ffe1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Load data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2bb448f-7a10-4c80-a4ef-275493a093d6"}}},{"cell_type":"code","source":["from sklearn.datasets.california_housing import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport pandas as pd\nnp.random.seed(0)\n\ncal_housing = fetch_california_housing()\n\n# split 80/20 train-test\nX_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n                                                        cal_housing.target,\n                                                        test_size=0.2,\n                                                        random_state=1)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3be32f19-3267-4662-9d76-ad6d1c8e52bc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Spark DataFrame\n\nLet's concatenate our features and label, then create a Spark DataFrame from our Pandas DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18541a51-e6d2-4bf8-b1dc-ded56d5a68e0"}}},{"cell_type":"code","source":["data = pd.concat([pd.DataFrame(X_train, columns=cal_housing.feature_names), pd.DataFrame(y_train, columns=[\"label\"])], axis=1)\ntrainDF = spark.createDataFrame(data)\ndisplay(trainDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4865c4d3-16d0-43ae-872d-c954552910f1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Create Dense Vectors for Features"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a5dad4e-ff21-461b-8a87-c01538fb3344"}}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nvecAssembler = VectorAssembler(inputCols=cal_housing.feature_names, outputCol=\"features\")\nvecTrainDF = vecAssembler.transform(trainDF).select(\"features\", \"label\")\ndisplay(vecTrainDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1844382e-0d84-44eb-95ba-b93566ea083a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Array\n\nPetastorm requires an Array as input, not a Vector. Let's register a UDF in Scala and invoke it from Python for optimal performance."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9233c1e0-92a5-47f4-a3b1-01f0d6992022"}}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.linalg.Vector\nval toArray = udf { v: Vector => v.toArray }\nspark.udf.register(\"toArray\", toArray)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8b08c84-c162-4726-a6d9-c97650b35913"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Save Data \n\nLet's write our DataFrame out as a parquet files to DBFS."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a70bf09c-9c34-4993-89d9-61351c088a0e"}}},{"cell_type":"code","source":["file_path = f\"{workingDir}/deep-learning/petastorm.parquet\"\nvecTrainDF.selectExpr(\"toArray(features) AS features\", \"label\").repartition(8).write.mode(\"overwrite\").parquet(file_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37502c50-186f-434d-88ca-e42bff97b974"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Remove committed/started metadata\n\nPetastorm + Horovod do not work if you leave the committed/started metadata files in our Parquet folder. We will need to remove them."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5b01d71-e622-409e-aaab-6feb2bebc41b"}}},{"cell_type":"code","source":["[dbutils.fs.rm(i.path) for i in dbutils.fs.ls(file_path) if (\"_committed_\" in i.name) | (\"_started_\" in i.name)]\n\ndisplay(dbutils.fs.ls(file_path))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88ba460e-0c4e-4fdd-b438-971bbd154896"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Define Model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5a2efbd-bcb7-4bf2-b1d2-7bad30a93fc9"}}},{"cell_type":"code","source":["import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import models, layers\ntf.set_random_seed(42)\n\ndef build_model():\n  from tensorflow.keras import models, layers\n  model = models.Sequential()\n  model.add(layers.Dense(20, input_dim=8, activation='relu'))\n  model.add(layers.Dense(20, activation='relu'))\n  model.add(layers.Dense(1, activation='linear'))\n  return model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d34792ba-59fe-4c8b-a3ec-629419c2c013"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Single Node\n\nDefine shape of the input tensor and output tensor and fit the model (on the driver). We need to use Petastorm's [make_batch_reader](https://petastorm.readthedocs.io/en/latest/api.html#petastorm.reader.make_batch_reader) to create an instance of Reader for reading batches out of a non-Petastorm Parquet store."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58f9a627-6319-4c08-a228-01e4692d339d"}}},{"cell_type":"code","source":["from petastorm import make_batch_reader\nfrom petastorm.tf_utils import make_petastorm_dataset\n\nabs_file_path = file_path.replace(\"dbfs:/\", \"/dbfs/\")\n\nwith make_batch_reader(\"file://\" + abs_file_path, num_epochs=None) as reader: \n  dataset = make_petastorm_dataset(reader).map(lambda x: (tf.reshape(x.features, [-1,8]), tf.reshape(x.label, [-1,1])))\n  model = build_model()\n  optimizer = keras.optimizers.Adam(lr=0.001)\n  model.compile(optimizer=optimizer,\n                loss='mse',\n                metrics=['mse'])\n  model.fit(dataset, steps_per_epoch=10, epochs=10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f09c533b-74d2-4dd9-90b5-1c5bbcdfe91f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Horovod\n\nLet's do the same thing, but let's add in Horovod for distributed model training."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66421107-b288-4a09-9af3-3f2ad9f4b268"}}},{"cell_type":"code","source":["import horovod.tensorflow.keras as hvd\n\ndef run_training_horovod():\n  # Horovod: initialize Horovod.\n  hvd.init()\n  with make_batch_reader(\"file://\" + abs_file_path, num_epochs=None, cur_shard=hvd.rank(), shard_count= hvd.size()) as reader:\n    dataset = make_petastorm_dataset(reader).map(lambda x: (tf.reshape(x.features, [-1,8]), tf.reshape(x.label, [-1,1])))\n    model = build_model()\n    from tensorflow.keras import optimizers\n    optimizer = optimizers.Adam(lr=0.001*hvd.size())\n    optimizer = hvd.DistributedOptimizer(optimizer)\n    model.compile(optimizer=optimizer,\n                  loss='mse',\n                  metrics=['mse'])\n    history = model.fit(dataset, steps_per_epoch=10, epochs=10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7f468e8-d0fa-45c2-9763-fa680c2b4ac0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Train on driver"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7dfbf04-82cd-4634-8e06-553c309b38cd"}}},{"cell_type":"code","source":["from sparkdl import HorovodRunner\nhr = HorovodRunner(np=-1)\nhr.run(run_training_horovod)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1f3449e-ff6d-4d54-b00e-92ab9715cbec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Better Horovod"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4000258e-3110-43c7-8bcc-3c5efe06fa78"}}},{"cell_type":"code","source":["import horovod.tensorflow.keras as hvd\n\n\ndbutils.fs.rm(f\"{ml_working_path}/petastorm_checkpoint_weights.ckpt\", True)\ndef run_training_horovod():\n  # Horovod: initialize Horovod.\n  hvd.init()\n  with make_batch_reader(\"file://\" + abs_file_path, num_epochs=None, cur_shard=hvd.rank(), shard_count=hvd.size()) as reader:\n    dataset = make_petastorm_dataset(reader).map(lambda x: (tf.reshape(x.features, [-1,8]), tf.reshape(x.label, [-1,1])))\n    model = build_model()\n    from tensorflow.keras import optimizers\n    optimizer = optimizers.Adam(lr=0.001*hvd.size())\n    optimizer = hvd.DistributedOptimizer(optimizer)\n    model.compile(optimizer=optimizer,\n                  loss='mse',\n                  metrics=['mse'])\n    checkpoint_dir = f\"{ml_working_path}/petastorm_checkpoint_weights.ckpt\"\n    callbacks = [\n    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n    hvd.callbacks.MetricAverageCallback(),\n    hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\", patience=10, verbose=1)\n    ]\n\n    if hvd.rank() == 0:\n      callbacks.append(tf.keras.callbacks.ModelCheckpoint(checkpoint_dir, save_weights_only=True))\n  \n    history = model.fit(dataset, steps_per_epoch=10, epochs=10, callbacks=callbacks)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13d660b9-81a8-4007-bf16-1a49aa9aadc0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import horovod.tensorflow.keras as hvd\nfrom sparkdl import HorovodRunner\nhr = HorovodRunner(np=-1)\nhr.run(run_training_horovod)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5413b23f-12c3-44f5-91f9-32f3d2b5526d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Run on all workers"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17701fb1-c3dc-4f7e-aa4c-6ffead923de6"}}},{"cell_type":"code","source":["from sparkdl import HorovodRunner\nhr = HorovodRunner(np=0)\nhr.run(run_training_horovod)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a697a54-5692-4729-bc51-1fdc83863801"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2. Horovod Petastorm","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3013059022292622}},"nbformat":4,"nbformat_minor":0}
